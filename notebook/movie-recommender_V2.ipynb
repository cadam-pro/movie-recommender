{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e421005c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb2403",
   "metadata": {},
   "source": [
    "# üé¨ Movies Data Pipeline\n",
    "Ce notebook contient un pipeline de traitement des donn√©es du fichier `TMDB_all_movies.csv`, dans le cadre d'un projet de data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5d5f0",
   "metadata": {},
   "source": [
    "## 1. Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "967c1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Movie recommender\").getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "\n",
    "df = spark.read.csv(\"../data/TMDB_all_movies.csv\", header=True, inferSchema=False, sep=\",\", quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3531c",
   "metadata": {},
   "source": [
    "## 2. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13124b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      " |-- vote_count: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- cast: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- director_of_photography: string (nullable = true)\n",
      " |-- writers: string (nullable = true)\n",
      " |-- producers: string (nullable = true)\n",
      " |-- music_composer: string (nullable = true)\n",
      " |-- imdb_rating: string (nullable = true)\n",
      " |-- imdb_votes: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- vote_average: float (nullable = true)\n",
      " |-- vote_count: float (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      " |-- revenue: float (nullable = true)\n",
      " |-- runtime: float (nullable = true)\n",
      " |-- budget: float (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- cast: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- writers: string (nullable = true)\n",
      " |-- music_composer: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.isna().sum()\n",
    "# df.isnull().sum() avec spark\n",
    "\n",
    "# Type de chaque colonne :\n",
    "    # vote_average: float\n",
    "    # vote_count: int\n",
    "    # release_date: date\n",
    "    # revenue: float\n",
    "    # runtime: float\n",
    "    # budget: float\n",
    "    # popularity: float\n",
    "    # genres, production_countries, production_companies, spoken, cast, director, writers: string / one-hot encoded ?\n",
    "\n",
    "df = df.withColumn(\"vote_average\", df[\"vote_average\"].try_cast(\"double\")) \\\n",
    "    .withColumn(\"vote_count\", df[\"vote_count\"].try_cast(\"double\")) \\\n",
    "    .withColumn(\"release_date\", df[\"release_date\"].cast(\"date\")) \\\n",
    "    .withColumn(\"revenue\", df[\"revenue\"].try_cast(\"double\")) \\\n",
    "    .withColumn(\"runtime\", df[\"runtime\"].try_cast(\"double\")) \\\n",
    "    .withColumn(\"budget\", df[\"budget\"].try_cast(\"double\")) \\\n",
    "    .withColumn(\"popularity\", df[\"popularity\"].try_cast(\"double\"))\n",
    "\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.select([pyspark.sql.functions.count(pyspark.sql.functions.when(pyspark.sql.functions.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53ae968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A faire :\n",
    "\n",
    "# enlever toutes les lignes qui ne sont pas en release\n",
    "# ensuite enlever la colonne \"status\"\n",
    "# enlever imdb_id, tagline, director_of_photography, producers, imdb_rating, imdb_votes,\n",
    "# drop les lignes qui n'ont pas d'overview\n",
    "\n",
    "# Enlever les donn√©es non important pour le calcul et les mettre de coter pour plus tard\n",
    "  # id, title, original_title, poster_path\n",
    "\n",
    "# df[\"status\"].unique()\n",
    "# df[\"genres\"]\n",
    "\n",
    "# supprime toutes les lignes qui n'ont pas status release\n",
    "df = df.filter(df[\"status\"] == \"Released\")\n",
    "# supprime la colonne status\n",
    "df = df.drop(\"status\")\n",
    "# supprime les colonnes qui ne sont pas utiles pour le calcul\n",
    "df = df.drop(\"imdb_id\", \"tagline\", \"director_of_photography\", \"producers\", \"imdb_rating\", \"imdb_votes\")\n",
    "# supprime les lignes qui n'ont pas d'overview\n",
    "# df = df.filter(df[\"overview\"].isNotNull() & (df[\"overview\"] != \"\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9de93fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "914169"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.filter(df[\"overview\"].isNotNull() & (df[\"overview\"] != \"\"))\n",
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|count|\n",
      "+--------------------+-----+\n",
      "|       Heading South|    2|\n",
      "|                Nell|    3|\n",
      "|             Nemesis|   20|\n",
      "|          Der Tunnel|    6|\n",
      "|              Deszcz|    2|\n",
      "|         Deep Rising|    2|\n",
      "|    Straight to Hell|    2|\n",
      "|Dance with the Devil|    4|\n",
      "|              Room 6|    2|\n",
      "|              Heaven|   30|\n",
      "|                Silk|   14|\n",
      "|       Crossing Over|   10|\n",
      "|   What No One Knows|    2|\n",
      "|        The Big Bang|    6|\n",
      "|          Riverworld|    2|\n",
      "|       Sugar & Spice|    4|\n",
      "|            Deep Red|    4|\n",
      "|        Miracle Mile|    2|\n",
      "|             Larceny|    4|\n",
      "|     My Name Is Khan|    2|\n",
      "|        Generation X|    2|\n",
      "|      A Woman's Face|    3|\n",
      "|         It's a Gift|    2|\n",
      "|    La Vie de Boh√®me|    2|\n",
      "|             Amateur|   14|\n",
      "|              Crisis|   17|\n",
      "|  A Woman Is a Woman|    2|\n",
      "|       Natural Enemy|    2|\n",
      "|           Surprise!|   12|\n",
      "|Diary of a Chambe...|    3|\n",
      "|         Lesser Evil|    2|\n",
      "|                Goth|    3|\n",
      "|            On Guard|    4|\n",
      "|          Night Eyes|    3|\n",
      "|          Nightmares|   13|\n",
      "|A Christmas Proposal|    3|\n",
      "|   The Corn Is Green|    2|\n",
      "|           Stay Gold|    2|\n",
      "|            The City|   23|\n",
      "|    The Stool Pigeon|    3|\n",
      "|                   K|    9|\n",
      "|         Lovely Rita|    3|\n",
      "|           The Salon|    2|\n",
      "|        See Jane Run|    3|\n",
      "|    The Star Boarder|    2|\n",
      "|        Checking Out|    3|\n",
      "|                 Kin|   21|\n",
      "|Loves Me, Loves M...|    3|\n",
      "|         The Lullaby|    3|\n",
      "|   The Biscuit Eater|    2|\n",
      "|          Red Square|    5|\n",
      "|  Horowitz in Moscow|    2|\n",
      "|  The Murdered House|    2|\n",
      "|      Snowman's Land|    2|\n",
      "|       The Encounter|   20|\n",
      "|Revenge: A Love S...|    2|\n",
      "|      Return to Life|    6|\n",
      "|             Errance|    4|\n",
      "|               Virgo|    4|\n",
      "|     My Dear Brother|    2|\n",
      "|       The Beginning|   27|\n",
      "|            Disraeli|    4|\n",
      "|      An Early Frost|    3|\n",
      "|               Apart|   17|\n",
      "|      Doctor Faustus|    5|\n",
      "|Heart of the Country|    2|\n",
      "|     The Queen and I|    2|\n",
      "|           El Diablo|    3|\n",
      "|                Ritu|    2|\n",
      "|We Can't Go Home ...|    2|\n",
      "|      Serpent's Path|    2|\n",
      "|Origin of the Spe...|    3|\n",
      "|    The Man Upstairs|    5|\n",
      "|             Susanna|    5|\n",
      "|    Thunder Mountain|    3|\n",
      "|        Room to Move|    2|\n",
      "|           Naufragio|    2|\n",
      "|          Quo Vadis?|    6|\n",
      "|      The Young Ones|    4|\n",
      "|      The Dressmaker|    5|\n",
      "|              Easter|   13|\n",
      "|           Sweet Jam|    2|\n",
      "| Stranger at My Door|    2|\n",
      "|                Maze|    8|\n",
      "|    Blue Suede Shoes|    2|\n",
      "|              Keemat|    2|\n",
      "|          Autumn Sun|    3|\n",
      "|            The Moth|    8|\n",
      "|           On My Way|    7|\n",
      "|      Where Are You?|    8|\n",
      "|     Ain't She Sweet|    2|\n",
      "|Morning, Noon and...|    2|\n",
      "|     The Setting Sun|    2|\n",
      "|          The Farmer|   10|\n",
      "|                Utah|    3|\n",
      "|        Anna Lucasta|    2|\n",
      "|           Zero Zero|    3|\n",
      "|          Growing Up|   12|\n",
      "|   Under Texas Skies|    2|\n",
      "|    Sunday Afternoon|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 100 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "\n",
    "duplicate_titles = (\n",
    "    df.groupBy(\"title\") \\\n",
    "      .agg(count(\"*\").alias(\"count\"))\\\n",
    "      .filter(col(\"count\") > 1)\\\n",
    ")\n",
    "\n",
    "duplicate_titles.show(100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1409b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 15:45:43 ERROR Executor: Exception in task 0.0 in stage 97.0 (TID 204)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '28.0' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 15 in cell [108]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/07/07 15:45:43 WARN TaskSetManager: Lost task 0.0 in stage 97.0 (TID 204) (artefact-de-vm-alniang.europe-west1-d.c.data-engineering-461712.internal executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '28.0' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 15 in cell [108]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/07 15:45:43 ERROR TaskSetManager: Task 0 in stage 97.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-07-07 15:45:43.741\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '28.0' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 15 in cell [108]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o2455.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '28.0' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 15 in cell [108]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/alpha.niang/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/alpha.niang/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value '28.0' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 15 in cell [108]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 26\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvote_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvote_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# df.printSchema()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# df_null_production_votes = df.filter(\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     (col(\"production_companies\").isNull()) & (col(\"vote_count\") > 100)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;66;03m#  (expr(\"try_cast(vote_count as double) > 100.0\"))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mdf_null_production_votes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:316\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    309\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    310\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         },\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/movie-recommender_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value '28.0' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 15 in cell [108]\n"
     ]
    }
   ],
   "source": [
    "# Explorer en profondeur productuion_companies vide\n",
    "\n",
    "# df_null_production = df.filter(col(\"production_companies\").isNull())\n",
    "\n",
    "# df_null_production = df_null_production.filter(col(\"vote_count\").cast(\"int\") > 100 )\n",
    "\n",
    "# df_null_production.show(100, truncate=False)\n",
    "# import IntegerType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = df.withColumn(\"vote_count\", col(\"vote_count\").cast(\"double\").cast(\"int\"))\n",
    "\n",
    "# df.printSchema()\n",
    "# df_null_production_votes = df.filter(\n",
    "#     (col(\"production_companies\").isNull()) & (col(\"vote_count\") > 100)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#df_null_production_votes = df.filter(\n",
    " #   (col(\"production_companies\").isNull()) &\n",
    "  #  (expr(\"try_cast(vote_count as double) > 100.0\"))\n",
    "#)\n",
    "\n",
    "df_null_production_votes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccef99",
   "metadata": {},
   "source": [
    "## 3. Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c1138",
   "metadata": {},
   "source": [
    "## 4. Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f2d1f",
   "metadata": {},
   "source": [
    "## 5. Chargement en base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-recommender_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
