#!/usr/bin/env python

import os
import sys
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from google.cloud import storage
from pyspark.sql.types import StringType
# from pyspark.sql.functions import col
from pyspark.sql.functions import col, concat_ws


def validate_environment():
    """Valide les variables d'environnement n√©cessaires"""
    required_vars = ["GCS_BUCKET_NAME", "GCS_BLOB_NAME", "GOOGLE_APPLICATION_CREDENTIALS"]
    missing_vars = []
    
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"‚ùå Variables d'environnement manquantes : {', '.join(missing_vars)}")
        return False
    
    return True

def create_spark_session(key_path):
    """Cr√©e et configure la session Spark"""
    print("üöÄ Initialisation de SparkSession")
    
    spark = SparkSession.builder \
        .appName("Movie Recommender") \
        .config("spark.jars.packages", "com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.5") \
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
        .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", key_path) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    # R√©duire la verbosit√© des logs
    spark.sparkContext.setLogLevel("ERROR")
    
    return spark

def read_csv_file(spark, file_path, description):
    """Lit un fichier CSV avec gestion d'erreur"""
    try:
        print(f"üìÇ Lecture du fichier {description} : {file_path}")
        df = spark.read.csv(
            file_path, 
            header=True, 
            inferSchema=True, 
            sep=",", 
            quote='"', 
            escape='"', 
            multiLine=True
        )
        print(f"‚úÖ Fichier {description} charg√© avec succ√®s ({df.count()} lignes)")
        return df
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du fichier {description} : {e}")
        return None

def convert_to_string_type(df):
    """Convertit toutes les colonnes en StringType"""
    print("üß™ Conversion des colonnes en StringType")
    for col_name in df.columns:
        df = df.withColumn(col_name, col(col_name).cast(StringType()))
    return df

def cleanup_temp_files(bucket, temp_prefix):
    """Nettoie les fichiers temporaires"""
    print("üßπ Nettoyage des fichiers temporaires")
    try:
        blobs = list(bucket.list_blobs(prefix=temp_prefix))
        for blob in blobs:
            blob.delete()
        print(f"‚úÖ {len(blobs)} fichiers temporaires supprim√©s")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du nettoyage : {e}")

def copy_final_file(bucket, temp_prefix, final_path):
    """Copie le fichier final depuis le dossier temporaire"""
    print("üîç Recherche du fichier CSV g√©n√©r√© dans le dossier temporaire")
    
    try:
        blobs = list(bucket.list_blobs(prefix=temp_prefix))
        csv_blob = next((blob for blob in blobs if blob.name.endswith('.csv')), None)
        
        if csv_blob:
            print(f"‚úÖ Fichier CSV g√©n√©r√© trouv√© : {csv_blob.name}")
            print(f"üìÅ Copie vers : {final_path}")
            bucket.copy_blob(csv_blob, bucket, final_path)
            return True
        else:
            print("‚ùå Aucun fichier .csv trouv√© dans le dossier temporaire")
            return False
    except Exception as e:
        print(f"‚ùå Erreur lors de la copie du fichier final : {e}")
        return False

def load_and_merge_data():
    """Fonction principale pour charger et fusionner les donn√©es"""
    print("üì¶ Chargement des variables d'environnement")
    load_dotenv()
    
    # Validation des variables d'environnement
    if not validate_environment():
        return False
    
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    source_blob = os.getenv("GCS_BLOB_NAME")
    key_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    
    # V√©rification de l'existence du fichier de cl√©s
    if not os.path.exists(key_path):
        print(f"‚ùå Fichier de cl√©s GCS introuvable : {key_path}")
        return False
    
    # D√©finition des chemins
    gcs_path = f"gs://{bucket_name}/{source_blob}"
    local_path = "data/test.csv"
    temp_path = f"gs://{bucket_name}/temp/csv_output/"
    final_path = "movies_recommender.csv"
    
    # Initialisation de Spark
    spark = create_spark_session(key_path)
    
    try:
        # Lecture du fichier GCS
        df1 = read_csv_file(spark, gcs_path, "GCS")
        if df1 is None:
            return False
        
        # Lecture du fichier local
        df2 = read_csv_file(spark, local_path, "local")
        if df2 is None:
            return False
        
        # V√©rification de la compatibilit√© des sch√©mas
        if df1.columns != df2.columns:
            print("‚ö†Ô∏è Avertissement : Les sch√©mas des fichiers ne correspondent pas exactement")
            print(f"Colonnes GCS : {df1.columns}")
            print(f"Colonnes local : {df2.columns}")
        
        # Conversion en StringType
        df1 = convert_to_string_type(df1)
        df2 = convert_to_string_type(df2)
        
        # Fusion des DataFrames
        print("üîó Fusion des DataFrames")
        df = df1.union(df2)
        total_rows = df.count()
        print(f"üìä Nombre total de lignes fusionn√©es : {total_rows}")
        
        return df, spark
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la fusion des donn√©es : {e}")
        return None, None

    # finally:
    #     print("üõë Arr√™t de Spark")
    #     spark.stop()

def save_merged_data(df):
    """Enregistre le DataFrame fusionn√© dans GCS"""
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    temp_path = f"gs://{bucket_name}/temp/csv_output/"
    final_path = "movies_recommender.csv"
    
    print("üíæ √âcriture du DataFrame fusionn√© dans un r√©pertoire temporaire GCS")
    try:
        df.coalesce(1) \
          .write \
          .option("header", "true") \
          .mode("overwrite") \
          .csv(temp_path)
        print("‚úÖ Fichier temporaire √©crit dans GCS")

        client = storage.Client()
        bucket = client.bucket(bucket_name)

        if copy_final_file(bucket, 'temp/csv_output/', final_path):
            print(f"‚úÖ Fichier final sauvegard√© sous : gs://{bucket_name}/{final_path}")
            cleanup_temp_files(bucket, 'temp/csv_output/')
            return True
        else:
            print("‚ùå √âchec de la copie du fichier final")
            return False
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'√©criture dans GCS : {e}")

def save_cleaned_data(df):
    """Enregistre le DataFrame nettoy√© au format CSV dans GCS (en aplatissant les colonnes Array)"""
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    temp_path = f"gs://{bucket_name}/temp/cleaned_csv_output/"
    final_path = "cleaned_movies_recommender.csv"
    
    print("üíæ √âcriture du DataFrame nettoy√© dans un r√©pertoire temporaire GCS")

    try:
        # üîÑ Aplatir les colonnes de type Array
        array_cols = [f.name for f in df.schema.fields if f.dataType.simpleString().startswith("array")]
        for col_name in array_cols:
            df = df.withColumn(col_name, concat_ws(", ", col(col_name)))

        # üíæ Sauvegarder en CSV dans GCS (temporairement)
        df.coalesce(1) \
          .write \
          .option("header", "true") \
          .mode("overwrite") \
          .csv(temp_path)
        print("‚úÖ Fichier temporaire √©crit dans GCS")

        # üìÇ Client GCS pour g√©rer le renommage et nettoyage
        client = storage.Client()
        bucket = client.bucket(bucket_name)

        # üìã Copier le bon fichier CSV g√©n√©r√© dans le dossier racine du bucket
        if copy_final_file(bucket, 'temp/cleaned_csv_output/', final_path):
            print(f"‚úÖ Fichier final sauvegard√© sous : gs://{bucket_name}/{final_path}")
            cleanup_temp_files(bucket, 'temp/cleaned_csv_output/')
            return True
        else:
            print("‚ùå √âchec de la copie du fichier final")
            return False

    except Exception as e:
        print(f"‚ùå Erreur lors de l'√©criture dans GCS : {e}")
        return False


# def save_cleaned_data(df):
#     """Enregistre le DataFrame nettoy√© dans GCS"""
#     bucket_name = os.getenv("GCS_BUCKET_NAME")
#     temp_path = f"gs://{bucket_name}/temp/cleaned_csv_output/"
#     final_path = "cleaned_movies_recommender.csv"
    
#     print("üíæ √âcriture du DataFrame n√©ttoy√© dans un r√©pertoire temporaire GCS")
#     try:
#         df.coalesce(1) \
#           .write \
#           .option("header", "true") \
#           .mode("overwrite") \
#           .csv(temp_path)
#         print("‚úÖ Fichier temporaire √©crit dans GCS")

#         client = storage.Client()
#         bucket = client.bucket(bucket_name)

#         if copy_final_file(bucket, 'temp/csv_output/', final_path):
#             print(f"‚úÖ Fichier final sauvegard√© sous : gs://{bucket_name}/{final_path}")
#             cleanup_temp_files(bucket, 'temp/cleaned_csv_output/')
#             return True
#         else:
#             print("‚ùå √âchec de la copie du fichier final")
#             return False
    
#     except Exception as e:
#         print(f"‚ùå Erreur lors de l'√©criture dans GCS : {e}")


# def load_data():
#     df, spark = load_and_merge_data()
#     if df is not None:
#         save_merged_data(df)
#         return df
#     else:
#         return None

# if __name__ == "__main__":
#     success = load_data()
#     sys.exit(0 if success else 1)